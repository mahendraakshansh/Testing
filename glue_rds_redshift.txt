from pyspark.sql import SparkSession
from pyspark.sql.functions import col,count

spark = SparkSession.builder.appName('Credit card').master('local[*]')\
            .config('spark.jars','s3://akshansh-bucket/jars/redshift-jdbc42-2.1.0.29.jar')\
            .getOrCreate()

empDf = spark.read.format('csv').option('header','true')\
            .option('delimiter',',')\
            .option('inferSchema','true')\
            .load('s3://akshansh-bucket/emp/emp.csv')

deptdf = spark.read.format('jdbc')\
            .option('url','jdbc:mysql://saif-db-instnc.cdo7vhlkey3k.us-east-1.rds.amazonaws.com:3306/akshansh_db?useSSL=False')\
            .option('driver','com.mysql.jdbc.Driver')\
            .option('user','admin')\
            .option('password','Welcome123')\
            .option('dbtable','dep')\
            .load()
            
mergedf = empDf.join(deptdf,deptdf.deptno==empDf.deptid,"inner")
filterdf = mergedf.filter(col('loc')=='CHICAGO')

filterdf.write.format('jdbc').mode('overwrite')\
            .option('url','jdbc:redshift://akshansh-cluster.cdj17lrr42z5.us-east-1.redshift.amazonaws.com:5439/akshansh_db')\
            .option('driver','com.amazon.redshift.jdbc.Driver')\
            .option('user','awsuser')\
            .option('password','Welcome123')\
            .option('dbtable','chicago_emp')\
            .save()



